{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Exploration into Theano\n",
    "\n",
    "This notebook aims to explore the implementation of a simple neural network supported by Theano, a powerful Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently.\n",
    "\n",
    "This introduction borrows from [this](http://outlace.com/Beginner-Tutorial-Theano/) theano introductory tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet as nnet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll set up our input vector and output scalar in proper Theano syntax. We're going to symbolically define two Theano variables called x and y. We'll build a network that emulates an XOR gate with 2 input units (+ a bias), 2 hidden units (+ a bias), and 1 output unit. So our x variable will always be a 2-element vector (e.g. [0,1]) and our y variable will always be a scalar and is our expected value for each pair of x values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.dvector()\n",
    "y = T.dscalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a Python function that will be a matrix multiplier and sigmoid function - our layer - so it will accept an x vector (and concatenate in a bias value of 1) and a w weight matrix, multiply them, and then run them through a sigmoid function. Theano has the sigmoid function built in the nnet class that we imported above. We'll use this function as our basic layer output function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(x, w):\n",
    "    b = np.array([1], dtype=theano.config.floatX)\n",
    "    new_x = T.concatenate([x, b])\n",
    "    m = T.dot(w.T, new_x) # theta1: 3x3 * x: 3x1 = 3x1 ;;; theta2: 1x4 * 4x1\n",
    "    h = nnet.sigmoid(m)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to concatenate a scalar value of 1 to our 1-dimensional vector x, we create a numpy array with a single element (1), and explicitly pass in the dtype parameter to make it a float64 and compatible with our Theano vector variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and implement our gradient descent function. This is where the power of Theano truly shines.  We're just going to have a function that defines a learning rate alpha and accepts a cost/error expression and a weight matrix. It will use Theano's grad() function to compute the gradient of the cost function with respect to the given weight matrix and return an updated weight matrix (*this is what the* `wrt` *parameter does*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grad_desc(cost, theta):\n",
    "    alpha = 0.1 # learning rate\n",
    "    return theta - (alpha * T.grad(cost, wrt=theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize some initial weight matrices with Theano. Since our weight matrices will take on definite values, they're not going to be represented as Theano variables, they're going to be defined as Theano's shared variable. A shared variable is what we use for variables that will take on a definite value, but that we also want to update. \n",
    "\n",
    "Note: the alpha or b (the bias term) aren't defined as shared variables, instead hard-coded them as strict values because they will never be updated/modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta1 = theano.shared(np.array(np.random.rand(3,3), dtype=theano.config.floatX)) # randomly initialize\n",
    "theta2 = theano.shared(np.array(np.random.rand(4,1), dtype=theano.config.floatX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've defined our two weight matrices for our 3 layer network and initialized them using numpy's random class. Again we specifically define the dtype parameter so it will be a float64, compatible with our Theano `dscalar` and `dvector` variable types.\n",
    "\n",
    "Here's where the cool stuff really comes in. We can start actually doing our computations for each layer in the network. Of course, we'll start by computing the hidden layer's output using our previously defined layer function, and pass in the Theano `x` variable we defined above and our `theta1` matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hid1 = layer(x, theta1) # hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for our final output layer. Notice how we use the `T.sum()` function on the outside which is the same as numpy's sum(). This is only because Theano will complain if you don't make it explicitly clear that our output is returning a scalar and not a matrix. Our matrix dimensional analysis is sure to return a `1x1` single element vector but we need to convert it to a scalar since we're substracting out 1 from y in our cost expression that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out1 = T.sum(layer(hid1, theta2)) # output layer\n",
    "fc = (out1 - y)**2 # cost expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! We're now going to compile two Theano functions. One will be our cost expression (for training), and the other will be our output layer expression (to run the network forward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = theano.function(inputs=[x, y], outputs=fc, updates=[\n",
    "        (theta1, grad_desc(fc, theta1)),\n",
    "        (theta2, grad_desc(fc, theta2))])\n",
    "run_forward = theano.function(inputs=[x], outputs=out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Theano function call looks a bit different from the first example - the additional updates parameter is included here. `updates` allows us to update shared variables according to an expression. `updates` expects a list of two tuples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "updates=[(shared_variable, update_value), ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of each tuple can be an expression or function that returns the new value we want to update the first part to. In our case, we have two shared variables we want to update, theta1 and theta2 and we want to use our grad_desc function to give us the updated data. Of course our grad_desc function expects two arguments, a cost function and a weight matrix, so we pass those in. fc is our cost expression. So every time we invoke/call the cost function that we've compiled with Theano, it will also update our shared variables according to our grad_desc rule. Pretty convenient!\n",
    "Additionally, we've compiled a run_forward function just so we can run the network forward and make sure it has trained properly. We don't need to update anything there.\n",
    "Now let's define our training data and setup a for loop to iterate through our training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.684592952473\n",
      "Cost: 0.240345575766\n",
      "Cost: 0.214414377688\n",
      "Cost: 0.102904356009\n",
      "Cost: 0.037652665605\n",
      "Cost: 0.0342444561011\n",
      "Cost: 0.0172451533626\n",
      "Cost: 0.0100416983002\n",
      "Cost: 0.00682883406448\n",
      "Cost: 0.00509501671538\n",
      "Cost: 0.00403101716551\n",
      "Cost: 0.00331882842844\n",
      "Cost: 0.00281182555068\n",
      "Cost: 0.00243401463605\n",
      "Cost: 0.00214240821583\n",
      "Cost: 0.00191099295178\n",
      "Cost: 0.00172316316614\n",
      "Cost: 0.00156784933121\n",
      "Cost: 0.00143740347523\n",
      "Cost: 0.00132638035932\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[0,1],[1,0],[1,1],[0,0]]).reshape(4,2) # training data X\n",
    "exp_y = np.array([1, 1, 0, 0]) # training data Y\n",
    "cur_cost = 0\n",
    "for i in range(10000):\n",
    "    for k in range(len(inputs)):\n",
    "        cur_cost = cost(inputs[k], exp_y[k]) #call our Theano-compiled cost function, it will auto update weights\n",
    "    if i % 500 == 0: #only print the cost every 500 epochs/iterations (to save space)\n",
    "        print('Cost: %s' % (cur_cost,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magic! We just trained a neural network that will act like an XOR gate! The cost consistently decreased, so it looks like our gradient descent implementation is functioning correctly.\n",
    "\n",
    "Right on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.970755280855\n",
      "0.0303932468439\n",
      "0.970320060519\n",
      "0.0351420873692\n"
     ]
    }
   ],
   "source": [
    "#Training done! Let's test it out\n",
    "print(run_forward([0,1]))\n",
    "print(run_forward([1,1]))\n",
    "print(run_forward([1,0]))\n",
    "print(run_forward([0,0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
