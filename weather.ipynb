{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "curr_dir = os.path.dirname('__file__')\n",
    "file_path = os.path.join(curr_dir, \"temps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(file_path, 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    data = list(reader)\n",
    "\n",
    "data = data[1:-3]\n",
    "for row in data:\n",
    "    row[1] = float(row[1].replace(\"?\", \"\"))\n",
    "df = pd.DataFrame(data, columns=['Date', 'Temperature'])\n",
    "df = pd.DataFrame(df.Temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _load_data(data, n_prev = 100):  \n",
    "    \"\"\"\n",
    "    data should be pd.DataFrame()\n",
    "    \"\"\"\n",
    "\n",
    "    docX, docY = [], []\n",
    "    for i in range(len(data)-n_prev):\n",
    "        docX.append(data.iloc[i:i+n_prev].as_matrix())\n",
    "        docY.append(data.iloc[i+n_prev].as_matrix())\n",
    "    print(docY)\n",
    "    alsX = np.array(docX)\n",
    "    alsY = np.array(docY).ravel()\n",
    "\n",
    "    return alsX, alsY\n",
    "\n",
    "def train_test_split(df, test_size=0.1):  \n",
    "    \"\"\"\n",
    "    This just splits data to training and testing parts\n",
    "    \"\"\"\n",
    "    ntrn = round(len(df) * (1 - test_size))\n",
    "\n",
    "    X_train, y_train = _load_data(df.iloc[0:ntrn])\n",
    "    X_test, y_test = _load_data(df.iloc[ntrn:])\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = train_test_split(df)  # retrieve data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By setting the first and second dimensions to None, we allow\n",
    "# arbitrary minibatch sizes with arbitrary sequence lengths.\n",
    "# The number of feature dimensions is 2, as described above.\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None, 1))\n",
    "# This input will be used to provide the network with masks.\n",
    "# Masks are expected to be matrices of shape (n_batch, n_time_steps);\n",
    "# both of these dimensions are variable for us so we will use\n",
    "# an input shape of (None, None)\n",
    "l_mask = lasagne.layers.InputLayer(shape=(None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All gates have initializers for the input-to-gate and hidden state-to-gate\n",
    "# weight matrices, the cell-to-gate weight vector, the bias vector, and the nonlinearity.\n",
    "# The convention is that gates use the standard sigmoid nonlinearity,\n",
    "# which is the default for the Gate class.\n",
    "gate_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "    b=lasagne.init.Constant(0.))\n",
    "\n",
    "cell_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "    # Setting W_cell to None denotes that no cell connection will be used.\n",
    "    W_cell=None, b=lasagne.init.Constant(0.),\n",
    "    # By convention, the cell nonlinearity is tanh in an LSTM.\n",
    "    nonlinearity=lasagne.nonlinearities.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Our LSTM will have 10 hidden/cell units\n",
    "N_HIDDEN = 10\n",
    "l_lstm = lasagne.layers.recurrent.LSTMLayer(\n",
    "    l_in, N_HIDDEN,\n",
    "    # We need to specify a separate input for masks\n",
    "    mask_input=l_mask,\n",
    "    # Here, we supply the gate parameters for each gate\n",
    "    ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "    cell=cell_parameters, outgate=gate_parameters,\n",
    "    # We'll learn the initialization and use gradient clipping\n",
    "    learn_init=True, grad_clipping=100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The \"backwards\" layer is the same as the first,\n",
    "# except that the backwards argument is set to True.\n",
    "l_lstm_back = lasagne.layers.recurrent.LSTMLayer(\n",
    "    l_in, N_HIDDEN, ingate=gate_parameters,\n",
    "    mask_input=l_mask, forgetgate=gate_parameters,\n",
    "    cell=cell_parameters, outgate=gate_parameters,\n",
    "    learn_init=True, grad_clipping=100., backwards=True)\n",
    "# We'll combine the forward and backward layer output by summing.\n",
    "# Merge layers take in lists of layers to merge as input.\n",
    "l_sum = lasagne.layers.ElemwiseSumLayer([l_lstm, l_lstm_back])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, retrieve symbolic variables for the input shape\n",
    "n_batch, n_time_steps, n_features = l_in.input_var.shape\n",
    "# Now, squash the n_batch and n_time_steps dimensions\n",
    "l_reshape = lasagne.layers.ReshapeLayer(l_sum, (-1, N_HIDDEN))\n",
    "# Now, we can apply feed-forward layers as usual.\n",
    "# We want the network to predict a single value, the sum, so we'll use a single unit.\n",
    "l_dense = lasagne.layers.DenseLayer(\n",
    "    l_reshape, num_units=1, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "# Now, the shape will be n_batch*n_timesteps, 1.  We can then reshape to\n",
    "# n_batch, n_timesteps to get a single value for each timstep from each sequence\n",
    "l_out = lasagne.layers.ReshapeLayer(l_dense, (n_batch, n_time_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Symbolic variable for the target network output.\n",
    "# It will be of shape n_batch, because there's only 1 target value per sequence.\n",
    "target_values = T.vector('target_output')\n",
    "# This matrix will tell the network the length of each sequences.\n",
    "# The actual values will be supplied by the gen_data function.\n",
    "mask = T.matrix('mask')\n",
    "\n",
    "# lasagne.layers.get_output produces an expression for the output of the net\n",
    "network_output = lasagne.layers.get_output(l_out)\n",
    "# The value we care about is the final value produced for each sequence\n",
    "# so we simply slice it out.\n",
    "predicted_values = network_output[:, -1]\n",
    "# Our cost will be mean-squared error\n",
    "cost = T.mean((predicted_values - target_values)**2)\n",
    "# Retrieve all parameters from the network\n",
    "all_params = lasagne.layers.get_all_params(l_out)\n",
    "# Compute adam updates for training\n",
    "updates = lasagne.updates.adam(cost, all_params)\n",
    "# Theano functions for training and computing cost\n",
    "train = theano.function(\n",
    "    [l_in.input_var, target_values, l_mask.input_var],\n",
    "    cost, updates=updates)\n",
    "compute_cost = theano.function(\n",
    "    [l_in.input_var, target_values, l_mask.input_var], cost)\n",
    "\n",
    "# We'll use this \"validation set\" to periodically check progress\n",
    "X_val, y_val, mask_val = gen_data()\n",
    "\n",
    "# We'll train the network with 10 epochs of 100 minibatches each\n",
    "NUM_EPOCHS = 10\n",
    "EPOCH_SIZE = 100\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for _ in range(EPOCH_SIZE):\n",
    "        X, y, m = gen_data()\n",
    "        train(X, y, m)\n",
    "    cost_val = compute_cost(X_val, y_val, mask_val)\n",
    "    print(\"Epoch {} validation cost = {}\".format(epoch + 1, cost_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
