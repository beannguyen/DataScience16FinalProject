{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing an LSTM RNN with Lasagne\n",
    "\n",
    "This notebook aims to serve as an extra helper guide to accompany lstm_text_gen.py, included in the same directory. Here, we'll walk through the different key components and concepts involved. If you want to try it out, download a source text, call it `source.txt`, and then finally run `lstm_text_gen.py`. The outputs will be logged in a file called lstm.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Where do we even start?\n",
    "When tasked with the challenge of implementing an LSTM RNN, a good place to start would be checking out what people have done in the past, and perhaps working from there. A great resource I found initially was [this implementation](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py) of close to what we want in Keras. It's based off of an architecture described in this other amazing and fun blog post - [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). I would highly recommend checking both of these off.\n",
    "\n",
    "Starting here will give us a good jumping off point with some existing pointers on how to structure the shape of the network, and what the inputs and outputs might look like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs, Shape, and more!\n",
    "Let's start by defining our goal here. We're trying to develop a character-level recurrent neural network using hidden layers composed of long short term memory elements, also known as LSTM. We'll be feeding in sequences of characters represented as one-hot vectors, and then using these inputs to predict the next character. The [Unreasonable Effectiveness](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) does a great job covering this concept. \n",
    "\n",
    "A quick example - \n",
    "\n",
    "With a sequence length of 5, an input and output might look like - \n",
    "INPUT: 'M', 'y', ' ', 'n', 'a'\n",
    "OUTPUT: 'm'\n",
    "\n",
    "Note: the major difference between this and a real implementation is in the one-hot encoding of each character.\n",
    "\n",
    "What about shape? For most RNNs, there seems to be somewhat of a consensus around the internet that 2-3 hidden layers is appropriate. As for the size of each hidden layer, we'll be choosing 512 hidden nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and Representation\n",
    "First, we'll need to get our data in the right shape. Let's take a look at the `gen_data` function defined in `lstm_text_gen.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_data(p, batch_size=BATCH_SIZE, data=in_text, return_target=True):\n",
    "    '''\n",
    "    Produces a semi-redundant batch of training samples from the location 'p' in the provided string (data).\n",
    "\n",
    "    Inputs: \n",
    "        p (int): Position in the input text to start at\n",
    "        batch_size (int): The number of frame shifts to make/training samples to make\n",
    "        data (str): The input corpus\n",
    "        return_target (boolean): Flag to determine whether to return the target vector\n",
    "\n",
    "    Returns:\n",
    "        x (np array): Input data matrix that takes the shape of (batch_size x SEQ_LENGTH x vocab_size)\n",
    "        y (np array): Output data representing one-hot encoding of guessed character\n",
    "    '''\n",
    "\n",
    "    x = np.zeros((batch_size,SEQ_LENGTH,vocab_size))\n",
    "    y = np.zeros(batch_size)\n",
    "\n",
    "    for n in range(batch_size):\n",
    "        ptr = n\n",
    "        for i in range(SEQ_LENGTH):\n",
    "            x[n,i,char_to_ix[data[p+ptr+i]]] = 1.\n",
    "        if(return_target):\n",
    "            y[n] = char_to_ix[data[p+ptr+SEQ_LENGTH]]\n",
    "    return x, np.array(y,dtype='int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through what's going on here. We take in an input text (represented as a string), and starting at an index `p` in the text, create `BATCH_SIZE` samples of length `SEQ_LENGTH`. Instead of the character itself, we choose the one-hot representation. Finally, the target is taken to be the next character past the end of the sequence. \n",
    "\n",
    "The result of all of this? A training matrix of size [batch_size, SEQ_LENGTH, vocab_size], and the corresponding target characters. Pretty neat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Up a Network\n",
    "\n",
    "Now, let's go through the process of building up the network. We'll have four layers in total, which will be built with the python library Lasagne. In order, we'll have:\n",
    "\n",
    "1. Input layer\n",
    "2. LSTM layer 1\n",
    "3. LSTM layer 2\n",
    "4. Dense layer (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A quick aside on Lasagne\n",
    "With Lasagne, creating a NN is as simple as compiling an input layer, middle (hidden layers) - of which there are many options, and the output. Whatâ€™s great here is that Lasagne already has built in implementations for many different kinds of neural-net layers, from our barebones layer to a convolutional or LSTM layer.  From there, you define your cost function, optimizer (e.g. stochastic gradient descent), and train it up! Here are some good resources to get started with Lasagne -> [here](http://lasagne.readthedocs.org/en/latest/index.html), [here](https://github.com/Lasagne/Recipes/tree/master/tutorials), and finally [here](https://github.com/craffel/Lasagne-tutorial/blob/master/examples/tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Layer\n",
    "First, we'll define our input layer. By setting the first two dimensions as None, we are allowing them to vary. They correspond to batch size and sequence length, so we will be able to feed in batches of varying size with sequences of varying length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1 -> Build the input layer of the network. \n",
    "# Recurrent layers expect shape (batch size, SEQ_LENGTH, num_features)\n",
    "l_in = lasagne.layers.InputLayer(shape=(None, None, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Layers\n",
    "Next, we'll define our two recurrent layers. \n",
    "\n",
    "We clip the gradients at GRAD_CLIP to prevent the problem of exploding gradients. I'll leave the specifics of the problems of exploding/disappearing gradients to the resources we've mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2 -> Build LSTM layer with takes l_in as input layer + clip gradient\n",
    "l_forward_1 = lasagne.layers.LSTMLayer(\n",
    "    l_in, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "    nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "l_forward_2 = lasagne.layers.LSTMLayer(\n",
    "    l_forward_1, N_HIDDEN, grad_clipping=GRAD_CLIP,\n",
    "    nonlinearity=lasagne.nonlinearities.tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Layers\n",
    "This is where things get a little complicated. The l_forward layer creates an output of dimension (batch_size, SEQ_LENGTH, N_HIDDEN). Since we are only interested in the final prediction, we isolate that quantity and feed it to the next layer. We can do this using a Lasagne SliceLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 3 -> Feed the output from the recurrent LSTM layers into a traditional Dense Layer\n",
    "# We only care about the final prediction here, so we isolate that and feed it into the next layer\n",
    "# The output of the sliced layer will then be of size (batch_size, N_HIDDEN)\n",
    "l_forward_slice = lasagne.layers.SliceLayer(l_forward_2, -1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we send the output from the SliceLayer through a traditional Dense layer using a softmax nonlinearity to get a probability distribution of the outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4 -> Sliced output passed through softmax nonlinearity to create probability distribution of the prediction\n",
    "# The output of this stage is (batch_size, vocab_size)\n",
    "l_out = lasagne.layers.DenseLayer(l_forward_slice,\n",
    "                                  num_units=vocab_size,\n",
    "                                  W = lasagne.init.Normal(),\n",
    "                                  nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilation and Cost\n",
    "\n",
    "The last step before start training is to get the actual output from the network, create a cost expression to evaluate the 'goodness' of the network, and provide an update function to perform gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Theano tensor for the targets\n",
    "target_values = T.ivector('target_output')\n",
    "\n",
    "# lasagne.layers.get_output produces a variable for the output of the net\n",
    "network_output = lasagne.layers.get_output(l_out)\n",
    "\n",
    "# The loss function is calculated as the mean of the (categorical) cross-entropy between the prediction and target.\n",
    "cost = T.nnet.categorical_crossentropy(network_output,target_values).mean()\n",
    "\n",
    "# Retrieve all parameters from the network\n",
    "all_params = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "\n",
    "# Compute AdaGrad updates for training\n",
    "logging.info(\"Computing updates ...\")\n",
    "updates = lasagne.updates.adagrad(cost, all_params, LEARNING_RATE)\n",
    "\n",
    "# Theano functions for training and computing cost\n",
    "logging.info(\"Compiling functions ...\")\n",
    "train = theano.function([l_in.input_var, target_values], cost, updates=updates, allow_input_downcast=True)\n",
    "compute_cost = theano.function([l_in.input_var, target_values], cost, allow_input_downcast=True)\n",
    "\n",
    "# Generate probabilities used to generate text from the network given the current state and seed text input\n",
    "probs = theano.function([l_in.input_var],network_output,allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Text\n",
    "Let's write a function to generate text given the current state of the network. When we're training the network, we'll be calling this function periodically to get an idea for what the network is outputting. Let's walk through how this process is going to work.\n",
    "\n",
    "We'll start with a seed generation text of at least length SEQ_LENGTH, from which we'll be building text. First, we'll take this and encode it to the one-hot representation we've been using. \n",
    "\n",
    "Then, we'll use the LSTM to predict the next character and store it in a dynamic list `sample_ix`. This character will be taken from the output using the `probs` function we defined above. We'll pick the one with the highest probability.\n",
    "\n",
    "Finally, we construct a new sequence of characters using all but the first character of the original sequence and the character we just guessed. This becomes our new generation input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(N=200):\n",
    "        '''\n",
    "        This function uses the user-provided string \"generation_phrase\" and current state of the RNN generate text.\n",
    "        The function works in three steps:\n",
    "        1. Convert string set in \"generation_phrase\" (which must be over SEQ_LENGTH characters long) \n",
    "           to encoded format. \n",
    "        2. Use LSTM to predict the next character and store it in a (dynamic) list sample_ix using the 'probs'\n",
    "           function which was compiled above.\n",
    "        3. Construct a new sequence using all but first characters of the provided string and the \n",
    "           predicted character. This sequence is then used to generate yet another character.\n",
    "           \n",
    "        This process continues for \"N\" characters. \n",
    "        '''\n",
    "\n",
    "        assert(len(generation_phrase)>=SEQ_LENGTH)\n",
    "        sample_ix = []\n",
    "        x,_ = gen_data(len(generation_phrase)-SEQ_LENGTH, 1, generation_phrase,0)\n",
    "\n",
    "        for i in range(N):\n",
    "            # Pick the character that got assigned the highest probability\n",
    "            ix = np.argmax(probs(x).ravel())\n",
    "            sample_ix.append(ix)\n",
    "            x[:,0:SEQ_LENGTH-1,:] = x[:,1:,:]\n",
    "            x[:,SEQ_LENGTH-1,:] = 0\n",
    "            x[0,SEQ_LENGTH-1,sample_ix[-1]] = 1. \n",
    "\n",
    "        random_snippet = generation_phrase + ''.join(ix_to_char[ix] for ix in sample_ix)    \n",
    "        logging.info(\"----\\n %s \\n----\" % random_snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Network\n",
    "\n",
    "Almost there! The last step is to train the network. To train, we simply iteratively call our `train` function defined above and update the gradients the appropriate amount. At a calcualted interval, we print out the prediction of the network, in addition to the current loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for it in xrange(data_size * num_epochs / BATCH_SIZE):\n",
    "    generate_text() # Generate text using the p^th character as the start. \n",
    "\n",
    "    avg_cost = 0;\n",
    "    for i in range(PRINT_FREQ):\n",
    "        x,y = gen_data(p)\n",
    "\n",
    "        p += SEQ_LENGTH + BATCH_SIZE - 1 \n",
    "        if(p+BATCH_SIZE+SEQ_LENGTH >= data_size):\n",
    "            logging.info('Carriage Return')\n",
    "            p = 0;\n",
    "\n",
    "        avg_cost += train(x, y)\n",
    "    logging.info(\"Epoch {} average loss = {}\".format(i*1.0*PRINT_FREQ/data_size*BATCH_SIZE, avg_cost / PRINT_FREQ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
